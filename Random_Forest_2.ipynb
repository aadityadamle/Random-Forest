{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random-Forest-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadityadamle/Random-Forest/blob/main/Random_Forest_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80aHLtEjjld"
      },
      "source": [
        "                                                          REFERENCE\n",
        "\n",
        "http://mystatisticsblog.blogspot.com/2017/12/building-random-forest-classifier-from.html\n",
        "\n",
        "https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynsw2m0whDwR"
      },
      "source": [
        "                                           RANDOM FOREST IMPLEMENTATION \n",
        "\n",
        "\n",
        "*   Random forest is a machine learning algorithm that produces, a great result most of the time. It is also one of the most used algorithms, because of its simplicity and diversity\n",
        "* It is a supervised learning algorithm.\n",
        "* It is an ensemble of decision trees .bagging increases the overall result.\n",
        "* It can be used for both classification and regression problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KETMfuvR297"
      },
      "source": [
        "                                            IMPORTING THE LIBRARIES:\n",
        "\n",
        "* **randrange** is a function used to call out any random number between the start and stop which are the input features of the function.\n",
        "* **Pandas** is a famous library that provides various data structures and operations for manipulating numerical data and time series. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppi7mqPRL0yv"
      },
      "source": [
        "from random import randrange\n",
        "import pandas as pd\n",
        "import math\n",
        "from math import sqrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHVoS5pz_kdl"
      },
      "source": [
        "\n",
        "*   load the dataset into a dataframe using pandas.\n",
        "*   converted the dataframe into list.\n",
        "\n",
        "                                                    ABOUT DATASET:\n",
        "\n",
        "* Cardiovascular diseases (CVDs) are the number 1 cause \n",
        "of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\n",
        "\n",
        "* Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n",
        "\n",
        "*   People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZvaaUzSM_7E",
        "outputId": "f844fe17-b8b1-4d2e-f825-a2884fc7a975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=8cf39f3339051664892aad95e529cf33d43b8cc3706950bb3c23c9a90f617009\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE116uRkNREr"
      },
      "source": [
        "import wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVlT2et39YAF"
      },
      "source": [
        "dset = pd.read_csv(\"/datasets_727551_1263738_heart_failure_clinical_records_dataset.csv\")\n",
        "dset = dset.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myktloxYCj5n"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "*  Defining a function which takes in input: the dataset, index of a column and certain value(value)\n",
        "*  we iterate through the rows,if the row's value corresponding to the column is greater than (value) we append the row to the right list, elseto the left list.\n",
        "*   this we we are able to separate the dataset into 2 parts based on the column's value of each row w.r.t the value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQNAW9EoNo7r"
      },
      "source": [
        "def testsplit(index, value, dset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dset:\n",
        "\t\tif row[index] > value:\n",
        "\t\t\tright.append(row)\n",
        "\t\telse:\n",
        "\t\t\tleft.append(row)\n",
        "\treturn left, right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8yvLLUv7lhg"
      },
      "source": [
        "                                                        CLASS COUNT\n",
        "\n",
        "\n",
        "\n",
        "*   given a group we define a function which takes input the group and return a list which contains the number of elements under each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9ErwGzu027P"
      },
      "source": [
        "def class_counts(group):\n",
        "    counts = {}  \n",
        "    for row in group:\n",
        "        label = row[-1]\n",
        "        if label not in counts:\n",
        "            counts[label] = 0\n",
        "        counts[label] += 1\n",
        "    return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On-SC3a22jMJ"
      },
      "source": [
        "                                                             ENTROPY\n",
        "\n",
        "*   entropy is a measure of disorder.\n",
        "*   entropy is an indicator of how messy the data is.\n",
        "*   we use entropy as a measure to maximize the purity of the groups as much as possible each time you create a new node of the tree.\n",
        "*   At each branching, we want to decrease the entropy, so this quantity is computed before the cut and after the cut.\n",
        "*   more the entropy decreases better the split. \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TdklbosXp3T"
      },
      "source": [
        "def entropy(group):\n",
        "    entries = class_counts(group)\n",
        "    avg_entropy = 0\n",
        "    size = float(len(group))\n",
        "    for label in entries:\n",
        "        prob = entries[label] / size\n",
        "        avg_entropy = avg_entropy + (prob * math.log(prob, 2))\n",
        "    return -1*avg_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EdtKabnqIPr"
      },
      "source": [
        "                                                    INFORMATION GAIN\n",
        "\n",
        "* Information gain gives us a value which determines how much it is helpful to split a group.higher value indicates that the group should be split \n",
        "* It is defined as : \n",
        "  \n",
        "   (entropy of parent group) -  (sum of entropies of child groups)\n",
        "\n",
        "*   initially we calculate entropy of the whole dataset as save the value in IG.then we iterste over the groups and sum over their individual entropy and substract from the IG calculated initially .\n",
        "                  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_rB5LGBnmRl"
      },
      "source": [
        "def Information_gain(groups):\n",
        "    all_data = [row for group in groups for row in group]\n",
        "    total_samples = sum([len(group) for group in groups])\n",
        "    IG = entropy(all_data)\n",
        "    for group in groups:\n",
        "        IG -= entropy(group)*len(group)/total_samples\n",
        "    return IG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niZGB3BOvVwz"
      },
      "source": [
        "                                                         GET SPLIT\n",
        "\n",
        "                                    \n",
        "\n",
        "* here we check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make.\n",
        "* This is an exhaustive and greedy algorithm\n",
        "  \n",
        "  WORKING:\n",
        "* a list of features is created and all the index of dataset is appended in it.\n",
        "\n",
        "*   taking a column into consideration , we check the best split iterating over each row in the column using test split wherein we take the row's index value as threshold value for splitting.\n",
        "\n",
        "* this whole process is iterated for every column\n",
        "* IG value for each iteration is recorded.the row of a particular index about which we get highest IG is considered the best split .\n",
        "* The index ,row[index] value and the groups created are given as output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEydCZNe9WFy"
      },
      "source": [
        "def get_split(dset, number_of_features):\n",
        "  classval = list(set(row[-1] for row in dset))\n",
        "  split_value, max_IG, split_groups = 0., -1., None \n",
        "  features = list()\n",
        "  while len(features) < number_of_features:\n",
        "    index = randrange(len(dset[0])-1)\n",
        "    if index not in features:\n",
        "       features.append(index)\n",
        "  for index in features:\n",
        "    for row in dset:\n",
        "      groups = testsplit(index, row[index], dset)\n",
        "      IG = Information_gain(groups)\n",
        "      if IG > max_IG:\n",
        "        updated_index, split_value, max_IG, split_groups = index ,row[index], IG, groups\n",
        "  return {'index': updated_index,'value': split_value,'groups': groups}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr6HrI8o3-Vf"
      },
      "source": [
        "        \n",
        "\n",
        "* **to_terminal** function returns the class with highest number of elements in the group as output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MNkulMKN82d"
      },
      "source": [
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgONvmjx7_I0"
      },
      "source": [
        "                                                    TERMINATION OF TREE\n",
        "\n",
        "* we create a function to decide when to stop growing a tree.\n",
        "* Once a maximum depth of the tree is met, we must stop splitting adding new nodes\n",
        "* Once at or below minimum node records, we must stop splitting and adding new nodes.\n",
        "* if we choose a split in which all rows belong to one group then in  this case, we will be unable to continue splitting and adding child nodes.\n",
        "* splitting of nodes takes place only when none of the above conditions is satisfied.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoNa3MKHOGmy"
      },
      "source": [
        "def split(node, max_depth, min_size, number_of_features, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        " \t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "  # check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right, number_of_features)\n",
        "\t\tsplit(node['right'], max_depth, min_size, number_of_features, depth+1)\n",
        "\t # process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left, number_of_features)\n",
        "\t\tsplit(node['left'], max_depth, min_size, number_of_features, depth+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky6oBLQcKT0f"
      },
      "source": [
        "                                                        BUILD TREE\n",
        "\n",
        "                                     \n",
        "\n",
        "* using the get split we first get the groups\n",
        "* then using split function to split nodes as much as possible till full tree is created.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fcK3YS8OKjK"
      },
      "source": [
        "def build_tree(train, max_depth, min_size, number_of_features):\n",
        "\troot = get_split(train, number_of_features)\n",
        "\tsplit(root, max_depth, min_size, number_of_features, 1)\n",
        "\treturn root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf8UPTePMy46"
      },
      "source": [
        "                                                PREDICTION\n",
        " We must check if a child node is either a terminal value to be returned as the prediction, or if it is a dictionary node containing another level of the tree to beconsidered. \n",
        " we implement this using the shown  recursive function                                              "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNjFWhXgONOK"
      },
      "source": [
        "def predict(node, row):\n",
        "\tif row[node['index']] > node['value']:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']\n",
        "\telse:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evRmG0tNarO9"
      },
      "source": [
        "\n",
        "\n",
        "*   we then create a function to choose the prediction made by all the trees. \n",
        "*   this is done by counting the number of same predictions and giving as output the value that has been predicted maximum number of times by the trees.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LYsp_iBOTY-"
      },
      "source": [
        "def bagging_predict(trees, row):\n",
        "\tpredictions = [predict(tree, row) for tree in trees]\n",
        "\treturn max(set(predictions), key=predictions.count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkDW4LrIbYrX"
      },
      "source": [
        "                                                     RANDOM FOREST \n",
        "\n",
        "\n",
        "\n",
        "* this is the combination of all the functions made till now.\n",
        "* we create a list of trees using the train set and predict all their results on the test set which are taken as input parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGEm9v7lXK1T"
      },
      "source": [
        "def random_forest(train, test, max_depth, min_size, n_trees, number_of_features):\n",
        "\ttrees = list()\n",
        "\tfor i in range(n_trees):\n",
        "\t\ttrain_set = list(train)\n",
        "\t\ttree = build_tree(train_set, max_depth, min_size, number_of_features)\n",
        "\t\ttrees.append(tree)\n",
        "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
        "\treturn(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUDt7fgbcTuO"
      },
      "source": [
        "\n",
        "\n",
        "* defining a function to calculate the accuracy of our prediction.\n",
        "* we compare the actual and predicted for all the prerdictions made and calculate the percentage of correct predictions made.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMzHTHWsXKAI"
      },
      "source": [
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect_prediction = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect_prediction += 1\n",
        "\treturn correct_prediction / float(len(actual)) * 100.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJBa-hLTdKbg"
      },
      "source": [
        "\n",
        "*   devising a function which takes in as input :the dataset ,the algorithm to be applied on the dataset,and the arguments of the algorithm, and returns the accuracy using the previously created functions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAMRwDwXVwAt"
      },
      "source": [
        "def algorithm_evaluation(dset, algorithm, n_folds, *args):\n",
        "  dset_split = list()\n",
        "  dset_copy = list(dset)\n",
        "  fold_size = int(len(dset) / n_folds)\n",
        "  test_set = list()\n",
        "  while len(test_set) < fold_size:\n",
        "\t\t\tindex = randrange(len(dset_copy))\n",
        "\t\t\ttest_set.append(dset_copy.pop(index))\n",
        "  train_set = list(dset_copy)\n",
        "  predicted = algorithm(train_set, test_set, *args)\n",
        "  actual = [row[-1] for row in test_set]\n",
        "  accuracy = accuracy_metric(actual, predicted)\n",
        "  return accuracy\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BP73T3IeTL4"
      },
      "source": [
        "                                                  TESTING ON DATASET                                 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNSA9o8OOlNW"
      },
      "source": [
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "number_of_features = int(sqrt(len(dset[0])-1))\n",
        "n_trees = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8BGCmvbHGPn",
        "outputId": "e428fd9a-587c-455f-d930-d8c52d04ea58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "score= algorithm_evaluation(dset, random_forest, n_folds, max_depth, min_size, n_trees, number_of_features)\n",
        "score\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}